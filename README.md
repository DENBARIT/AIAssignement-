### Extended LSTM Cell Implementation (From Scratch)

This project demonstrates a manual implementation of an Extended Long Short-Term Memory (LSTM) cell using NumPy, without relying on deep learning frameworks such as TensorFlow or PyTorch.

The purpose of this implementation is educational â€” to clearly show how LSTM gates and internal states work step-by-step.

###Project Objectives

Implement an LSTM cell from scratch

Understand the internal components of LSTM:

Forget gate

Input gate

Candidate cell state

Cell state

Output gate

Hidden state

Track and visualize how hidden and cell states evolve over time

Provide transparent numerical outputs for learning and analysis

###Key Features

Manual implementation of LSTM equations

Reproducible results using a fixed random seed

Step-by-step gate value inspection

Visualization of:

Hidden state (h)

Cell state (c)

Lightweight and easy to understand

### Technologies Used

Python 3

NumPy

Matplotlib
 ###Project Structure
.
â”œâ”€â”€ Lstm.py        # Main Python script
â”œâ”€â”€ README.md      # Project documentation
 How to Run the Code

1.Clone the repository
git clone https://github.com/your-username/your-repo-name.git
cd your-repo-name
2.Install dependencies
pip install numpy matplotlib
3.Run the script
python Lstm.py
 Example Input Sequence
inputs = [5,1.9,2]
The model processes each value sequentially and prints:

Hidden state

Cell state

Forget gate value

Input gate value

Output gate value

Candidate cell value

 Output Visualization

The script generates a plot showing:

Hidden State (h) over time

Cell State (c) over time

This helps visualize how memory is preserved and updated across time steps.

 Mathematical Overview

At each time step, the LSTM computes:

Forget gate

ğ‘“
ğ‘¡
=
ğœ
(
ğ‘Š
ğ‘“
[
â„
ğ‘¡
âˆ’
1
,
ğ‘¥
ğ‘¡
]
+
ğ‘
ğ‘“
)
f
t
	â€‹

=Ïƒ(W
f
	â€‹

[h
tâˆ’1
	â€‹

,x
t
	â€‹

]+b
f
	â€‹

)

Input gate

ğ‘–
ğ‘¡
=
ğœ
(
ğ‘Š
ğ‘–
[
â„
ğ‘¡
âˆ’
1
,
ğ‘¥
ğ‘¡
]
+
ğ‘
ğ‘–
)
i
t
	â€‹

=Ïƒ(W
i
	â€‹

[h
tâˆ’1
	â€‹

,x
t
	â€‹

]+b
i
	â€‹

)

Candidate cell

ğ‘
~
ğ‘¡
=
tanh
â¡
(
ğ‘Š
ğ‘
[
â„
ğ‘¡
âˆ’
1
,
ğ‘¥
ğ‘¡
]
+
ğ‘
ğ‘
)
c
~
t
	â€‹

=tanh(W
c
	â€‹

[h
tâˆ’1
	â€‹

,x
t
	â€‹

]+b
c
	â€‹

)

Cell state

ğ‘
ğ‘¡
=
ğ‘“
ğ‘¡
â‹…
ğ‘
ğ‘¡
âˆ’
1
+
ğ‘–
ğ‘¡
â‹…
ğ‘
~
ğ‘¡
c
t
	â€‹

=f
t
	â€‹

â‹…c
tâˆ’1
	â€‹

+i
t
	â€‹

â‹…
c
~
t
	â€‹


Output gate

ğ‘œ
ğ‘¡
=
ğœ
(
ğ‘Š
ğ‘œ
[
â„
ğ‘¡
âˆ’
1
,
ğ‘¥
ğ‘¡
]
+
ğ‘
ğ‘œ
)
o
t
	â€‹

=Ïƒ(W
o
	â€‹

[h
tâˆ’1
	â€‹

,x
t
	â€‹

]+b
o
	â€‹

)

Hidden state

â„
ğ‘¡
=
ğ‘œ
ğ‘¡
â‹…
tanh
â¡
(
ğ‘
ğ‘¡
)
h
t
	â€‹

=o
t
	â€‹

â‹…tanh(c
t
	â€‹

)

 ###Authors

Samuel Dessalegn â€” UGR/2304/14

Leul Gebremariam â€” UGR/3478/15

Yohannes Wale â€” UGR/1232/14

Yohannes Alemu â€” UGR/8644/15

###Academic Use

This project is suitable for:

Artificial Intelligence courses

Machine Learning fundamentals

Neural Networks assignments

Understanding LSTM internals conceptually

### License

This project is provided for educational purposes.
You may reuse or modify it with proper attribution.
